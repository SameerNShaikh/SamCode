# -*- coding: utf-8 -*-
"""MLCA1_SDGR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G912MRn6-KJCOAxhcJMvlnhuXFfB0guI

#Stochastic Gradient Descent Regressor(SGDR)


> To predict life expectancy from data provided by the WHO using the SGDR model

"""

################################################ Start of Un-optimised SGDR ####################################################

import numpy as np
import pandas as pd
import plotly.graph_objs as go
import plotly.figure_factory as ff
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

dataset = pd.read_csv("LifeExpectancy.csv")
#pd.set_option('display.max_columns', None)
#print(dataset.head())
#print(dataset.shape)
#print(dataset.info())
#print(dataset.describe())

dataset.replace(" ", np.nan, inplace = True)

"""check_missing = dataset.isnull()
#print(check_missing)
for column in check_missing.columns.values.tolist():
  print(column)
  print(check_missing[column].value_counts())
  print("")"""

# Replacing NaN values with the 0
dataset["Life expectancy"].replace(np.nan, 0, inplace=True)

dataset["Adult Mortality"].replace(np.nan, 0, inplace=True)

dataset["infant deaths"].replace(np.nan, 0, inplace=True)

dataset["Alcohol"].replace(np.nan, 0, inplace=True)

dataset["percentage expenditure"].replace(np.nan, 0, inplace=True)

dataset["Hepatitis B"].replace(np.nan, 0, inplace=True)

dataset["BMI"].replace(np.nan, 0, inplace=True)

dataset["Polio"].replace(np.nan, 0, inplace=True)

dataset["Total expenditure"].replace(np.nan, 0, inplace=True)

dataset["Diphtheria"].replace(np.nan, 0, inplace=True)

dataset["HIV/AIDS"].replace(np.nan, 0, inplace=True)

dataset["GDP"].replace(np.nan, 0, inplace=True)

dataset["Population"].replace(np.nan, 0, inplace=True)

dataset["thinness  1-19 years"].replace(np.nan, 0, inplace=True)

dataset["thinness 5-9 years"].replace(np.nan, 0, inplace=True)

dataset["Income composition of resources"].replace(np.nan, 0, inplace=True)

dataset["Schooling"].replace(np.nan, 0, inplace=True)

x = dataset.drop(['Country', 'Status','Life expectancy'], axis = 1)
y = dataset['Life expectancy']

sgdr = SGDRegressor(random_state = 1, penalty = None)
grid_param = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000]}

gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)

feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(x)

gd_sr.fit(X_scaled, y)

results = pd.DataFrame.from_dict(gd_sr.cv_results_)
#print("Cross-validation results:\n", results)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
#print("Best result: ", best_result)

# Implementing Regularization
# Tuning the SGDRegressor parameters 'eta0' (learning rate) and 'max_iter', along with the regularization parameter alpha using Grid Search
sgdr = SGDRegressor(random_state = 1, penalty = 'elasticnet', eta0=.01, max_iter=10000)
grid_param = {'alpha': [.0001, .001, .01, .1, 1], 'l1_ratio': [0,0.25,0.5,0.75,1]}

gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)

gd_sr.fit(X_scaled, y)

results = pd.DataFrame.from_dict(gd_sr.cv_results_)
print("Cross-validation results:\n", results)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best result: ", best_result)

yPred = gd_sr.predict(X_scaled)

print("Prediction\n", yPred)

################################################ End of Un-optimised SGDR ####################################################


################################################ Start of Optimised SGDR ####################################################

dataset = pd.read_csv("LifeExpectancy.csv")
#pd.set_option('display.max_columns', None)
#print(dataset.head())
#print(dataset.shape)
#print(dataset.info())
#print(dataset.describe())

dataset.Status = dataset.Status.astype('category').cat.codes

dataset.replace(" ", np.nan, inplace = True)

"""check_missing = dataset.isnull()
#print(check_missing)
for column in check_missing.columns.values.tolist():
  print(column)
  print(check_missing[column].value_counts())
  print("")"""


#Replacing NaN values with the mean of that column
avg_Le = dataset["Life expectancy"].mean(axis=0)
dataset["Life expectancy"].replace(np.nan, avg_Le, inplace=True)

avg_Am = dataset["Adult Mortality"].mean(axis=0)
dataset["Adult Mortality"].replace(np.nan, avg_Am, inplace=True)

avg_Id = dataset["infant deaths"].mean(axis=0)
dataset["infant deaths"].replace(np.nan, avg_Id, inplace=True)

avg_Alc = dataset["Alcohol"].mean(axis=0)
dataset["Alcohol"].replace(np.nan, avg_Alc, inplace=True)

avg_Pe = dataset["percentage expenditure"].mean(axis=0)
dataset["percentage expenditure"].replace(np.nan, avg_Pe, inplace=True)

avg_Hb = dataset["Hepatitis B"].mean(axis=0)
dataset["Hepatitis B"].replace(np.nan, avg_Hb, inplace=True)

avg_Bmi = dataset["BMI"].mean(axis=0)
dataset["BMI"].replace(np.nan, avg_Bmi, inplace=True)

avg_Po = dataset["Polio"].mean(axis=0)
dataset["Polio"].replace(np.nan, avg_Po, inplace=True)

avg_Te = dataset["Total expenditure"].mean(axis=0)
dataset["Total expenditure"].replace(np.nan, avg_Te, inplace=True)

avg_Dp = dataset["Diphtheria"].mean(axis=0)
dataset["Diphtheria"].replace(np.nan, avg_Dp, inplace=True)

avg_Ha = dataset["HIV/AIDS"].mean(axis=0)
dataset["HIV/AIDS"].replace(np.nan, avg_Ha, inplace=True)

avg_Gd = dataset["GDP"].mean(axis=0)
dataset["GDP"].replace(np.nan, avg_Gd, inplace=True)

avg_Pop = dataset["Population"].mean(axis=0)
dataset["Population"].replace(np.nan, avg_Pop, inplace=True)

avg_Th1 = dataset["thinness  1-19 years"].mean(axis=0)
dataset["thinness  1-19 years"].replace(np.nan, avg_Th1, inplace=True)

avg_Th2 = dataset["thinness 5-9 years"].mean(axis=0)
dataset["thinness 5-9 years"].replace(np.nan, avg_Th2, inplace=True)

avg_Ic = dataset["Income composition of resources"].mean(axis=0)
dataset["Income composition of resources"].replace(np.nan, avg_Ic, inplace=True)

avg_Sch = dataset["Schooling"].mean(axis=0)
dataset["Schooling"].replace(np.nan, avg_Sch, inplace=True)

#checking correlation and plotting a heatmap
corrs = dataset.corr()
figure = ff.create_annotated_heatmap(
    z=corrs.values,
    x=list(corrs.columns),
    y=list(corrs.index),
    annotation_text=corrs.round(2).values,
    showscale=True)
figure.show()

x = dataset.drop(['Country','Life expectancy', 'Income composition of resources','infant deaths','percentage expenditure','thinness 5-9 years'], axis = 1)
y = dataset['Life expectancy']

#Performing scaling
feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(x)

xTrain, xTest, yTrain, yTest = train_test_split(X_scaled, y, test_size = 0.3, random_state = 42)

sgdr = SGDRegressor(random_state = 1, penalty = None)
grid_param = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000]}

gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)

gd_sr.fit(xTrain, yTrain)

#cross validation before Regularization
crossval_results = pd.DataFrame.from_dict(gd_sr.cv_results_)
print("Cross-validation results:\n", crossval_results)

#Regularization
sgdr = SGDRegressor(random_state = 1, penalty = 'elasticnet', eta0=.01, max_iter=10000)
grid_param = {'alpha': [.0001, .001, .01, .1, 1], 'l1_ratio': [0,0.25,0.5,0.75,1]}

gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)

gd_sr.fit(xTrain, yTrain)

yPred = gd_sr.predict(xTest)

crossval_results = pd.DataFrame.from_dict(gd_sr.cv_results_)
print("Cross-validation results:\n", crossval_results)

best_parameters = gd_sr.best_params_
print("Best parameters: ", best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best result: ", best_result)

r2 = r2_score(yTest, yPred)
print("r2 Score: ", r2)

best_model = gd_sr.best_estimator_
print("Intercept: ", best_model.intercept_)

print(pd.DataFrame(zip(x.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

print("Prediction\n", yPred)

################################################ Start of Optimised SGDR ####################################################